1. Download wiki corpora: wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2
2. Use Gensim to preprocess the corpora 
    2.1. Preprocess documents and convert them into TF-IDF BOW vectors: make_wikicorpus.sh
        --> produces files: in data/
            - wiki_wordids.txt.bz2: mapping between words and their integer ids
            - wiki_bow.mm: bag-of-words (word counts) representation in Matrix Market format
            - wiki_bow.mm.index: index for wiki_bow.mm
            - wiki_bow.mm.metadata.cpickle: titles of documents
            - wiki_tfidf.mm: TF-IDF representation in Matrix Market format
            - wiki_tfidf.mm.index: index for wiki_tfidf.mm
            - wiki.tfidf_model: TF-IDF model
    2.2. Inspect words: 
        2.2.1. Extract words to inspect: get_200_word_types.sh
            --> produces file: data/selected_wordtypes.txt
        2.2.2. Inspect if the words are legit: inspect_words.sh
            -> takes files: data/selected_wordtypes.txt
            --> produces file: results/ratio_invalid_en_word_wiki.txt
3. Run LSA model: lsa.sh
    -> takes files: 
    --> produces files: 
4. Compute cosine distances
    4.0. Create a vocabulary with the 5000 most frequent words in the corpus (can be found in data/vocab.pkl)
    [4.1. Compute cosine distances between all words in the vocabulary
        --> 12497500 combinations]
    4.2. Compute cosine distances between words of the same length in the vocabulary (from 3 to 7 characters): cos_dist_wordlength.py
        Nb of words per word length
            3: 257
            4: 584
            5: 744
            6: 837
            7: 789
            --> total nb of words compared: 3211
5. Compute edit distance for each word pair for each word length: edit_dist.py
6. Compute Pearson correlations between cosine and edit distance: correlations_per_length.py
